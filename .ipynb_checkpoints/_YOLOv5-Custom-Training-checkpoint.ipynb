{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrsaDfdVHzxt"
   },
   "source": [
    "# Custom Training with YOLOv5\n",
    "\n",
    "In this tutorial, we use the coco128 dataset and train a custom YOLOv5 model to recognize the objects. To do so we will take the following steps:\n",
    "\n",
    "* Utilise the coco128 dataset that has images and labels\n",
    "* Train YOLOv5 to recognize the objects in our coco128 dataset\n",
    "* Evaluate our YOLOv5 model's performance\n",
    "* Run test inference to view our model at work\n",
    "\n",
    "![](https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/615627e5824c9c6195abfda9_computer-vision-cycle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNveqeA1KXGy"
   },
   "source": [
    "# Step 1: Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19977,
     "status": "ok",
     "timestamp": 1643387523767,
     "user": {
      "displayName": "Ed Harris",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgauMgl-5ozWp5ss2gkaI9BpkrKEh4hRbj7qoPlYA=s64",
      "userId": "12889952584970668414"
     },
     "user_tz": 0
    },
    "id": "kTvDNSILZoN9",
    "outputId": "17e4d03c-838b-403e-8b41-03d160e2088a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: 'yolov5'\n",
      "C:\\Users\\George Wager\\OneDrive - Harper Adams University\\Documents\\Implementing-YOLOv5\\yolov5\n",
      "Requirement already satisfied: opencv-python-headless in c:\\users\\george wager\\anaconda3\\envs\\yolov5\\lib\\site-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\george wager\\anaconda3\\envs\\yolov5\\lib\\site-packages (from opencv-python-headless) (1.23.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#clone YOLOv5 from github\n",
    "#!git clone https://github.com/ultralytics/yolov5\n",
    "%cd yolov5\n",
    "%pip install opencv-python-headless\n",
    "%pip install -qr requirements.txt\n",
    "%pip install -q roboflow\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from IPython.display import Image, clear_output\n",
    "\n",
    "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")\n",
    "\n",
    "# Some warnings okay as of 2022-01-28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7yAi9hd-T4B"
   },
   "source": [
    "# Step 2: Train Our Custom YOLOv5 model\n",
    "\n",
    "Here, we are able to pass a number of arguments:\n",
    "- **img:** define input image size\n",
    "- **--rect** ONLY use is images are rectangle\n",
    "- **batch:** determine batch size\n",
    "- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n",
    "- **data:** Our dataset locaiton is saved in the `dataset.location`\n",
    "- **weights:** specify a path to weights to start transfer learning from. Here we choose the generic COCO pretrained checkpoint.\n",
    "- **cache:** cache images for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/Implementing-YOLOv5/yolov5\n"
     ]
    }
   ],
   "source": [
    "%pwd\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112034,
     "status": "ok",
     "timestamp": 1643387857205,
     "user": {
      "displayName": "Ed Harris",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgauMgl-5ozWp5ss2gkaI9BpkrKEh4hRbj7qoPlYA=s64",
      "userId": "12889952584970668414"
     },
     "user_tz": 0
    },
    "id": "eaFNnxLJbq4J",
    "outputId": "9a8b3543-2850-44f4-bed3-b5cac13e42d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=../yolo-files/coco128.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=30, batch_size=16, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
      "YOLOv5 🚀 v6.1-25-gcaf7ad0 torch 1.10.2+cu102 CUDA:0 (Tesla T4, 15110MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs (RECOMMENDED)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt to yolov5s.pt...\n",
      "100%|██████████████████████████████████████| 14.1M/14.1M [00:00<00:00, 86.7MB/s]\n",
      "\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model Summary: 270 layers, 7235389 parameters, 7235389 gradients, 16.5 GFLOPs\n",
      "\n",
      "Transferred 349/349 items from yolov5s.pt\n",
      "Scaled weight_decay = 0.0005\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight (no decay), 60 weight, 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/home/studio-lab-user/Implementing-YOLOv5/coco128/labels/train2\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/studio-lab-user/Implementing-YOLOv5/coco128/labels/train2017.cache\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram): 100%|█████| 128/128 [00:00<00:00, 343.04it/s]\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/home/studio-lab-user/Implementing-YOLOv5/coco128/labels/train201\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100%|███████| 128/128 [00:00<00:00, 158.47it/s]\u001b[0m\n",
      "Plotting labels to runs/train/exp7/labels.jpg... \n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.18 anchors/target, 0.977 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mWARNING: Extremely small objects found: 16 of 929 labels are < 3 pixels in size\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 927 points...\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.6699: 100%|█| 1\u001b[0m\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 0.9935 best possible recall, 3.75 anchors past thr\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=416, metric_all=0.263/0.670-mean/best, past_thr=0.477-mean: 6,9, 16,14, 21,35, 55,47, 70,94, 80,188, 190,139, 216,249, 388,283\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n",
      "Image sizes 416 train, 416 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp7\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      0/29     1.63G   0.07692   0.05355   0.02513       250       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.404      0.537      0.476      0.234\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      1/29     2.15G    0.0584   0.05072   0.02575       155       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.538       0.58      0.609      0.359\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      2/29     2.15G   0.05394   0.05454   0.02069       279       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.656      0.574      0.658      0.391\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      3/29     2.15G   0.05116   0.05049   0.02239       200       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.687        0.6      0.676      0.403\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      4/29     2.15G   0.04888   0.04765   0.01943       221       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.684      0.612      0.695      0.404\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      5/29     2.15G   0.04927   0.04913   0.01743       188       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.706      0.672      0.714       0.44\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      6/29     2.15G   0.05194   0.04833   0.01821       227       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.769      0.607      0.692      0.377\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      7/29     2.15G   0.05524   0.04527   0.01754       239       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.747       0.61      0.676      0.359\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      8/29     2.15G   0.05215   0.04358   0.01417       241       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.787      0.636      0.723      0.417\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      9/29     2.15G   0.05101   0.04941   0.01532       219       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.746      0.654      0.722      0.396\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     10/29     2.15G   0.05131   0.04067   0.01545       190       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.709      0.635      0.689       0.38\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     11/29     2.15G    0.0525   0.04637   0.01543       255       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.735      0.657      0.722      0.395\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     12/29     2.15G   0.05165   0.04502   0.01374       306       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.725      0.685      0.738      0.407\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     13/29     2.15G   0.05182    0.0419   0.01445       240       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.661      0.722      0.735      0.407\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     14/29     2.15G   0.05009   0.04234   0.01372       230       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.667      0.762      0.766      0.444\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     15/29     2.15G   0.04881   0.04127   0.01442       271       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.761      0.725       0.77      0.442\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     16/29     2.15G   0.04651   0.04228   0.01303       216       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929       0.75      0.729      0.763      0.442\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     17/29     2.15G   0.04483   0.03904   0.01286       196       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.833      0.732        0.8       0.49\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     18/29     2.15G    0.0441   0.03835   0.01147       164       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.835      0.738        0.8      0.486\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     19/29     2.15G   0.04219   0.03314    0.0115       199       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.818      0.751      0.803      0.479\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     20/29     2.15G   0.04542   0.03987   0.01211       209       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.813      0.772      0.818      0.502\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     21/29     2.15G    0.0432   0.04072   0.01221       242       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.874      0.747       0.82      0.521\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     22/29     2.15G   0.04141   0.04566   0.01004       184       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.843      0.765      0.822      0.518\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     23/29     2.15G   0.04303   0.04085   0.01096       177       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.837      0.777      0.832       0.53\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     24/29     2.15G   0.04077   0.03535   0.01062       216       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.818      0.793      0.828      0.545\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     25/29     2.15G   0.04192   0.03858   0.01083       211       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.858       0.77      0.826      0.556\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     26/29     2.15G   0.04171   0.03849   0.01014       163       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.819      0.788      0.826      0.568\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     27/29     2.15G   0.04075   0.03785   0.01054       235       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929       0.82      0.782      0.828      0.576\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     28/29     2.15G   0.03981   0.03783  0.009744       149       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.842       0.78       0.83      0.583\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     29/29     2.15G   0.04105   0.03505   0.01075       279       416: 100%|███\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.851      0.777      0.836      0.584\n",
      "\n",
      "30 epochs completed in 0.018 hours.\n",
      "Optimizer stripped from runs/train/exp7/weights/last.pt, 14.8MB\n",
      "Optimizer stripped from runs/train/exp7/weights/best.pt, 14.8MB\n",
      "\n",
      "Validating runs/train/exp7/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 7225885 parameters, 0 gradients, 16.5 GFLOPs\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all        128        929      0.848      0.776      0.835      0.582\n",
      "              person        128        254      0.944      0.709      0.819      0.562\n",
      "             bicycle        128          6      0.776        0.5      0.739      0.436\n",
      "                 car        128         46       0.83      0.348      0.446      0.216\n",
      "          motorcycle        128          5      0.923          1      0.995      0.935\n",
      "            airplane        128          6      0.945          1      0.995      0.881\n",
      "                 bus        128          7          1        0.8      0.889      0.745\n",
      "               train        128          3      0.924          1      0.995      0.715\n",
      "               truck        128         12       0.92        0.5      0.644      0.406\n",
      "                boat        128          6      0.848        0.5      0.826      0.509\n",
      "       traffic light        128         14      0.779      0.256      0.326      0.232\n",
      "           stop sign        128          2      0.836          1      0.995      0.747\n",
      "               bench        128          9          1      0.884      0.968      0.498\n",
      "                bird        128         16      0.981          1      0.995      0.701\n",
      "                 cat        128          4      0.923          1      0.995      0.855\n",
      "                 dog        128          9      0.985          1      0.995      0.789\n",
      "               horse        128          2      0.836          1      0.995      0.821\n",
      "            elephant        128         17      0.932      0.941      0.947       0.71\n",
      "                bear        128          1      0.765          1      0.995      0.995\n",
      "               zebra        128          4      0.916          1      0.995      0.864\n",
      "             giraffe        128          9      0.964          1      0.995      0.817\n",
      "            backpack        128          6       0.88      0.833      0.836      0.429\n",
      "            umbrella        128         18      0.741      0.833      0.871      0.514\n",
      "             handbag        128         19      0.805      0.421       0.56      0.379\n",
      "                 tie        128          7      0.811      0.714      0.709      0.466\n",
      "            suitcase        128          4      0.878          1      0.995      0.703\n",
      "             frisbee        128          5      0.879        0.8        0.8      0.555\n",
      "                skis        128          1      0.908          1      0.995      0.697\n",
      "           snowboard        128          7      0.883      0.714      0.869      0.507\n",
      "         sports ball        128          6       0.91      0.667      0.761      0.376\n",
      "                kite        128         10      0.548        0.3      0.425      0.155\n",
      "        baseball bat        128          4      0.622        0.5      0.521      0.238\n",
      "      baseball glove        128          7      0.672      0.295      0.361      0.233\n",
      "          skateboard        128          5      0.675      0.834      0.898      0.527\n",
      "       tennis racket        128          7      0.647      0.429      0.565      0.375\n",
      "              bottle        128         18          1      0.329      0.681      0.375\n",
      "          wine glass        128         16      0.992      0.938      0.964      0.417\n",
      "                 cup        128         36      0.857      0.835      0.878      0.553\n",
      "                fork        128          6      0.966      0.667      0.752      0.448\n",
      "               knife        128         16      0.851      0.625        0.7      0.302\n",
      "               spoon        128         22       0.91        0.5      0.604      0.305\n",
      "                bowl        128         28      0.973      0.786      0.792      0.646\n",
      "              banana        128          1      0.781          1      0.995      0.697\n",
      "            sandwich        128          2       0.83          1      0.995      0.598\n",
      "              orange        128          4      0.705          1      0.945      0.592\n",
      "            broccoli        128         11      0.803      0.273      0.467      0.366\n",
      "              carrot        128         24      0.842      0.667      0.835      0.538\n",
      "             hot dog        128          2      0.843          1      0.995      0.945\n",
      "               pizza        128          5          1      0.899      0.995      0.879\n",
      "               donut        128         14      0.939      0.929      0.986      0.731\n",
      "                cake        128          4      0.872          1      0.995      0.877\n",
      "               chair        128         35      0.789      0.771      0.845      0.527\n",
      "               couch        128          6      0.842          1      0.995      0.682\n",
      "        potted plant        128         14      0.885          1       0.99      0.623\n",
      "                 bed        128          3      0.869          1      0.995      0.831\n",
      "        dining table        128         13      0.935      0.769      0.885      0.597\n",
      "              toilet        128          2      0.851          1      0.995      0.846\n",
      "                  tv        128          2      0.837          1      0.995      0.573\n",
      "              laptop        128          3          1      0.655      0.995      0.568\n",
      "               mouse        128          2      0.764        0.5      0.545      0.204\n",
      "              remote        128          8      0.688      0.625      0.607      0.339\n",
      "          cell phone        128          8      0.703      0.625      0.631      0.391\n",
      "           microwave        128          3      0.883          1      0.995      0.898\n",
      "                oven        128          5      0.762        0.8      0.938      0.637\n",
      "                sink        128          6      0.682        0.5      0.669      0.499\n",
      "        refrigerator        128          5      0.901          1      0.995      0.742\n",
      "                book        128         29          1      0.335      0.524      0.224\n",
      "               clock        128          9          1       0.98      0.995      0.741\n",
      "                vase        128          2      0.766          1      0.995      0.895\n",
      "            scissors        128          1      0.203      0.406      0.497      0.348\n",
      "          teddy bear        128         21      0.886      0.905      0.935       0.58\n",
      "          toothbrush        128          5      0.918          1      0.995      0.752\n",
      "Results saved to \u001b[1mruns/train/exp7\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train.py --img 416 --batch 16 --epochs 30 --data ../yolo-files/coco128.yaml --weights yolov5s.pt --cache # to run this make sure your\n",
    "# pwd is set in the \"yolov5\" folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: test test test\n",
    "Now test your model on some test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/studio-lab-user/Implementing-YOLOv5/yolov5'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set your pwd as the yolov5\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6956,
     "status": "ok",
     "timestamp": 1643388655456,
     "user": {
      "displayName": "Ed Harris",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgauMgl-5ozWp5ss2gkaI9BpkrKEh4hRbj7qoPlYA=s64",
      "userId": "12889952584970668414"
     },
     "user_tz": 0
    },
    "id": "TWjjiBcic3Vz",
    "outputId": "5059da10-f85a-43da-edfb-f8d897260353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/exp7/weights/best.pt'], source=../coco128/test, data=data/coco128.yaml, imgsz=[416, 416], conf_thres=0.1, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5 🚀 v6.1-25-gcaf7ad0 torch 1.10.2+cu102 CUDA:0 (Tesla T4, 15110MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 7225885 parameters, 0 gradients, 16.5 GFLOPs\n",
      "image 1/4 /home/studio-lab-user/Implementing-YOLOv5/coco128/test/Cthulhu_and_R'lyeh.jpg: 416x320 Done. (0.008s)\n",
      "image 2/4 /home/studio-lab-user/Implementing-YOLOv5/coco128/test/test1.jpg: 288x416 1 bear, Done. (0.008s)\n",
      "image 3/4 /home/studio-lab-user/Implementing-YOLOv5/coco128/test/test2.jpg: 288x416 1 elephant, Done. (0.008s)\n",
      "image 4/4 /home/studio-lab-user/Implementing-YOLOv5/coco128/test/test3.jpg: 416x288 1 cat, Done. (0.008s)\n",
      "Speed: 0.3ms pre-process, 7.7ms inference, 0.9ms NMS per image at shape (1, 3, 416, 416)\n",
      "Results saved to \u001b[1mruns/detect/exp5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights runs/train/exp7/weights/best.pt --img 416 --conf 0.1 --source ../coco128/test # make sure you have the\n",
    "#\"exp\" as the model you just ran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8dHcni6CJYt"
   },
   "source": [
    "# Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You've trained a custom YOLOv5 model to recognize your custom objects.\n",
    "\n",
    "To improve you model's performance, we recommend first interating on your datasets coverage and quality. See this guide for [model performance improvement](https://github.com/ultralytics/yolov5/wiki/Tips-for-Best-Training-Results).\n",
    "\n",
    "To deploy your model to an application, see this guide on [exporting your model to deployment destinations](https://github.com/ultralytics/yolov5/issues/251).\n",
    "\n",
    "Once your model is in production, you will want to continually iterate and improve on your dataset and model via [active learning](https://blog.roboflow.com/what-is-active-learning/)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "YOLOv5-Custom-Training.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/roboflow-ai/yolov5-custom-training-tutorial/blob/main/yolov5-custom-training.ipynb",
     "timestamp": 1643381752017
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
